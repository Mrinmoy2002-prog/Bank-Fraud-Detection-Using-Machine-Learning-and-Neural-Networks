# Bank-Fraud-Detection-Using-Machine-Learning-and-Neural-Networks
This project implements a Banking Fraud Detection System using Machine Learning and Deep Learning techniques. The system is designed to identify and categorize fraudulent transactions with high accuracy, assisting financial institutions in mitigating fraud risks effectively.

📌 Key Features
Machine Learning and Deep Learning Models:

Artificial Neural Networks (ANN)
Support Vector Machines (SVM)
K-Nearest Neighbors (KNN)
Model Evaluation:
Comprehensive evaluation to identify the best-performing model based on accuracy, precision, recall, and F1-score.
Risk Categorization:
Fraud levels are categorized into three distinct risk levels, enabling prioritized responses and better fraud management strategies.

🛠️ Technologies and Tools Used
Programming Language: Python
Libraries: NumPy, Pandas, Scikit-learn, TensorFlow/Keras, Matplotlib, Seaborn
Machine Learning Techniques: Supervised and Unsupervised Learning
Deep Learning Framework: TensorFlow/Keras

📝 Dataset
The dataset used includes features representing transaction attributes, customer behavior, and fraud indicators. Data preprocessing techniques were applied to handle imbalances, missing values, and feature scaling.

🚀 Objectives
Develop a robust system to detect fraudulent transactions accurately.
Evaluate and compare the performance of multiple models.
Provide actionable insights by categorizing fraud into low, medium, and high-risk levels.

📊 Workflow
Data Preprocessing: Cleaning, normalization, and handling class imbalances.
Model Development: Training multiple models and fine-tuning hyperparameters.
Evaluation: Selecting the best model based on performance metrics.
Risk Categorization: Using decision thresholds to classify fraud risk.

🤖 Results
The system achieved high accuracy and reliable fraud detection performance.
Fraud transactions were categorized into three risk levels for better prioritization.

💡 Future Enhancements
Integrate real-time transaction processing for dynamic fraud detection.
Explore advanced models such as Gradient Boosting or LSTM for sequential data.
Optimize for scalability and deployment on cloud platforms.
